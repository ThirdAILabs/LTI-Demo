{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip3 install thirdai --upgrade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from thirdai import neural_db as ndb, licensing\n",
    "licensing.activate(\"D0F869-B61466-6A28F0-14B8C6-0AC6C6-V3\")\n",
    "import pandas as pd\n",
    "import fitz \n",
    "from langchain.text_splitter import CharacterTextSplitter"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Process PDF files into CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_text_from_pdf(pdf_path):\n",
    "    doc = fitz.open(pdf_path)\n",
    "    text = \"\"\n",
    "    for page in doc:\n",
    "        text += page.get_text()\n",
    "    doc.close()\n",
    "    return text\n",
    "\n",
    "def save_chunks_to_csv(chunks, csv_path):\n",
    "    df = pd.DataFrame(chunks, columns=['Text'])\n",
    "    df.to_csv(csv_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    }
   ],
   "source": [
    "# specify the pdf paths here\n",
    "pdf_paths = []\n",
    "\n",
    "csv_files = []\n",
    "for pdf_path in pdf_paths:\n",
    "    csv_out_path = pdf_path.split(\".\")[0] + \".csv\"\n",
    "    csv_files.append(csv_out_path)\n",
    "    chunk_size = 1000\n",
    "    chunk_overlap = 100\n",
    "    text = extract_text_from_pdf(pdf_path)\n",
    "    splitter = CharacterTextSplitter(chunk_size=chunk_size,chunk_overlap=chunk_overlap,separator='\\n')\n",
    "\n",
    "    chunks = list(map(lambda x: x.page_content, splitter.create_documents([text])))\n",
    "    print(len(chunks))\n",
    "    save_chunks_to_csv(chunks, csv_out_path)\n",
    "print(csv_files)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train NeuralDB over generated files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocessed csv files\n",
    "csv_files = ['data/pfizer-20221231.csv',\n",
    "               'data/tsla-20231231.csv',\n",
    "               'data/msft-10-Q.csv',\n",
    "               'data/walmart-10k.csv',\n",
    "               'data/samsung-2022-10k.csv',\n",
    "               'data/apple-10k.csv',\n",
    "               'data/nvda-10k.csv',\n",
    "               'data/meta-10k.csv']\n",
    "csv_docs = [ndb.CSV(path=csv_file, strong_columns=['Text'], weak_columns=[], reference_columns=['Text']) for csv_file in csv_files]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to load a pretrained model, uncomment this\n",
    "# db = ndb.NeuralDB.from_checkpoint(\"lti_finetuned.ndb\")\n",
    "\n",
    "# training model from scratch\n",
    "db = ndb.NeuralDB()\n",
    "db.insert(csv_docs)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finetune the model over questions, paragraph pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "question_df = pd.read_csv(\"questions_large.csv\")\n",
    "import tqdm\n",
    "for csv_file in csv_files:\n",
    "    df = pd.read_csv(csv_file)\n",
    "    print(csv_file)\n",
    "    para = df['Text'].to_list()\n",
    "    temp_df = question_df[question_df['source']==csv_file]\n",
    "    for _, row in tqdm.tqdm(temp_df.iterrows(), total=len(temp_df)):\n",
    "        question = row['question']\n",
    "        db.associate(question, para[int(row['para_id'])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "db.save(\"lti_finetuned.ndb\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inference "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['OPENAI_API_KEY'] = \"\" # set OPENAI API Key here\n",
    "\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from paperqa.prompts import qa_prompt\n",
    "from paperqa.chains import make_chain\n",
    "from langchain.prompts import PromptTemplate\n",
    "llm = ChatOpenAI(\n",
    "    model_name='gpt-3.5-turbo', \n",
    "    temperature=0.1,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_references(query, radius=None):\n",
    "    search_results = db.search(query,top_k=3)\n",
    "    references = []\n",
    "    for result in search_results:\n",
    "        if (radius):\n",
    "            references.append(result.context(radius=radius))\n",
    "        else:\n",
    "            references.append(result.text)\n",
    "    return references\n",
    "\n",
    "def get_answer(query, references):\n",
    "    #uses default qa_prompt\n",
    "    qa_chain = make_chain(prompt=qa_prompt, llm=llm)\n",
    "    return qa_chain.run(question=query, context='\\n\\n'.join(references[:5]), answer_length=\"abt 50 words\")\n",
    "\n",
    "def get_answer_manual_prompt(query, references, prompt):\n",
    "    qa_chain = make_chain(prompt=prompt, llm=llm)\n",
    "    return qa_chain.run(question=query, context='\\n\\n'.join(references[:5]))\n",
    "    # can pass in manual prompt here\n",
    "    # these input variables would  need to be passed while calling qa_chain.run\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"What is the revenue of apple in year 2022\"\n",
    "\n",
    "references = get_references(query, radius=1)\n",
    "# print(references)\n",
    "answer = get_answer(query, references)\n",
    "\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"What is the revenue of apple in year 2022\"\n",
    "\n",
    "references = get_references(query)\n",
    "# print(references)\n",
    "\n",
    "# design your own prompt here\n",
    "# make sure to have two variables question and context as defined\n",
    "manual_prompt = (\n",
    "        PromptTemplate.from_template(\"Answer the question based on the following context\\n\")\n",
    "        + \"Question: {question}\"\n",
    "        + \"Context: {context}\"\n",
    "    ) \n",
    "print(get_answer_manual_prompt(query, references, manual_prompt))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
